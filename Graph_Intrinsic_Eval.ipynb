{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPYDMs5bz+Mcj12m25tG620",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vibhuverma17/COACH/blob/main/Graph_Intrinsic_Eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph-based Libraries\n",
        "!pip install networkx               # Graph manipulation\n",
        "!pip install node2vec               # Graph embeddings using Node2Vec\n",
        "!pip install torch-geometric        # Geometric deep learning library\n",
        "\n",
        "# Machine Learning Libraries\n",
        "!pip install scikit-learn           # General machine learning tools\n",
        "!pip install tensorflow             # Deep learning framework\n",
        "!pip install torch                  # PyTorch framework\n",
        "\n",
        "# Visualization Libraries\n",
        "!pip install matplotlib             # Data visualization\n",
        "\n",
        "# Knowledge Graphs and Optimization Libraries\n",
        "!pip install pykeen                 # Knowledge graph embeddings\n",
        "!pip install pykeops                # Optimized computation for large-scale problems\n",
        "\n",
        "# Additional Libraries\n",
        "!pip install numpy                  # Numerical computations\n",
        "!pip install hdbscan                 # Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN)"
      ],
      "metadata": {
        "id": "fj8ds4s10att"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA, NMF\n",
        "from sklearn.preprocessing import normalize, StandardScaler\n",
        "from node2vec import Node2Vec\n",
        "import hdbscan\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import from_networkx, to_networkx\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv, VGAE, TransformerConv, global_mean_pool\n",
        "from torch_geometric.datasets import Planetoid, Reddit\n",
        "from pykeops.torch import LazyTensor\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn.cluster import KMeans, SpectralClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "dataset = Planetoid(root='./data/CiteSeer', name='CiteSeer')\n",
        "data = dataset[0]\n",
        "\n",
        "G = to_networkx(data, to_undirected=True)\n",
        "\n",
        "print(\"Number of nodes:\", G.number_of_nodes())\n",
        "print(\"Number of edges:\", G.number_of_edges())\n",
        "\n",
        "subgraph = nx.subgraph(G, list(G.nodes))\n",
        "plt.figure(figsize=(10, 10))\n",
        "nx.draw(subgraph, node_size=10)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Function for clustering and evaluation\n",
        "def cluster_and_evaluate(embeddings, graph):\n",
        "    # Normalize embeddings for better clustering performance\n",
        "    normalized_embeddings = normalize(embeddings)\n",
        "\n",
        "    # Clustering: K-Means\n",
        "    kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "    kmeans_labels = kmeans.fit_predict(normalized_embeddings)\n",
        "\n",
        "    # Clustering: Spectral Clustering\n",
        "    spectral = SpectralClustering(n_clusters=5, affinity='nearest_neighbors', random_state=42)\n",
        "    spectral_labels = spectral.fit_predict(normalized_embeddings)\n",
        "\n",
        "    # Calculate Silhouette Score\n",
        "    silhouette_kmeans = silhouette_score(normalized_embeddings, kmeans_labels)\n",
        "    silhouette_spectral = silhouette_score(normalized_embeddings, spectral_labels)\n",
        "\n",
        "    # Modularity Calculation\n",
        "    def calculate_modularity(graph, labels):\n",
        "        # Create a dictionary to group nodes by their labels\n",
        "        communities = {i: [] for i in set(labels)}\n",
        "        for node, label in enumerate(labels):\n",
        "            communities[label].append(node)\n",
        "\n",
        "        # Convert to a list of sets for modularity calculation\n",
        "        community_list = [set(community) for community in communities.values()]\n",
        "\n",
        "        # Check if the communities form a valid partition\n",
        "        all_nodes = set(graph.nodes)\n",
        "        covered_nodes = set().union(*community_list)\n",
        "        if all_nodes != covered_nodes:\n",
        "            raise ValueError(\"Generated communities do not cover all nodes in the graph.\")\n",
        "\n",
        "        return nx.algorithms.community.modularity(graph, community_list)\n",
        "\n",
        "    modularity_kmeans = calculate_modularity(graph, kmeans_labels)\n",
        "    modularity_spectral = calculate_modularity(graph, spectral_labels)\n",
        "\n",
        "    return {\n",
        "        'Silhouette KMeans': silhouette_kmeans,\n",
        "        'Silhouette Spectral': silhouette_spectral,\n",
        "        'Modularity KMeans': modularity_kmeans,\n",
        "        'Modularity Spectral': modularity_spectral\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to convert PyTorch tensors to NumPy arrays\n",
        "def tensor_to_numpy(tensor):\n",
        "    if isinstance(tensor, torch.Tensor):\n",
        "        return tensor.detach().numpy()\n",
        "    return tensor\n",
        "\n",
        "# Updated function for clustering and evaluation\n",
        "def cluster_and_evaluate_tensor(embeddings, graph):\n",
        "    # Convert embeddings to NumPy if they are PyTorch tensors\n",
        "    embeddings = tensor_to_numpy(embeddings)\n",
        "\n",
        "    # Normalize embeddings for better clustering performance\n",
        "    normalized_embeddings = normalize(embeddings)\n",
        "\n",
        "    # Clustering: K-Means\n",
        "    kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "    kmeans_labels = kmeans.fit_predict(normalized_embeddings)\n",
        "\n",
        "    # Clustering: Spectral Clustering\n",
        "    spectral = SpectralClustering(n_clusters=5, affinity='nearest_neighbors', random_state=42)\n",
        "    spectral_labels = spectral.fit_predict(normalized_embeddings)\n",
        "\n",
        "    # Calculate Silhouette Score\n",
        "    silhouette_kmeans = silhouette_score(normalized_embeddings, kmeans_labels)\n",
        "    silhouette_spectral = silhouette_score(normalized_embeddings, spectral_labels)\n",
        "\n",
        "    # Modularity Calculation\n",
        "    def calculate_modularity(graph, labels):\n",
        "        # Create a dictionary to group nodes by their labels\n",
        "        communities = {i: [] for i in set(labels)}\n",
        "        for node, label in enumerate(labels):\n",
        "            communities[label].append(node)\n",
        "\n",
        "        # Convert to a list of sets for modularity calculation\n",
        "        community_list = [set(community) for community in communities.values()]\n",
        "\n",
        "        # Check if the communities form a valid partition\n",
        "        all_nodes = set(graph.nodes)\n",
        "        covered_nodes = set().union(*community_list)\n",
        "        if all_nodes != covered_nodes:\n",
        "            raise ValueError(\"Generated communities do not cover all nodes in the graph.\")\n",
        "\n",
        "        return nx.algorithms.community.modularity(graph, community_list)\n",
        "\n",
        "    modularity_kmeans = calculate_modularity(graph, kmeans_labels)\n",
        "    modularity_spectral = calculate_modularity(graph, spectral_labels)\n",
        "\n",
        "    return {\n",
        "        'Silhouette KMeans': silhouette_kmeans,\n",
        "        'Silhouette Spectral': silhouette_spectral,\n",
        "        'Modularity KMeans': modularity_kmeans,\n",
        "        'Modularity Spectral': modularity_spectral\n",
        "    }\n"
      ],
      "metadata": {
        "id": "xXvWeX_ew9ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `G` is your graph (NetworkX Graph object)\n",
        "\n",
        "# 1. Laplacian Eigenmaps\n",
        "L = nx.laplacian_matrix(G).toarray()\n",
        "n_components = min(L.shape[0], 64)\n",
        "pca = PCA(n_components=n_components)\n",
        "laplacian_embeddings = pca.fit_transform(L)\n",
        "\n",
        "# 2. HOPE (High-Order Proximity Embedding)\n",
        "A = nx.to_numpy_array(G)\n",
        "nmf = NMF(n_components=n_components, init='random', random_state=42)\n",
        "hope_embeddings = nmf.fit_transform(A)\n",
        "\n",
        "# 3. DeepWalk\n",
        "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
        "deepwalk_model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
        "deepwalk_embeddings = {node: deepwalk_model.wv[node] for node in G.nodes()}\n",
        "\n",
        "# 4. node2vec (Biased Random Walks)\n",
        "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, p=1, q=1, workers=4)\n",
        "node2vec_model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
        "node2vec_embeddings = {node: node2vec_model.wv[node] for node in G.nodes()}\n",
        "\n",
        "# 5. Struc2Vec (Simulated for illustration)\n",
        "struc2vec_embeddings = np.random.rand(len(G.nodes()), 64)\n",
        "\n",
        "\n",
        "# Output the embeddings for each method\n",
        "print(\"Laplacian Eigenmaps Embeddings: \", laplacian_embeddings.shape)\n",
        "print(\"HOPE Embeddings: \", hope_embeddings.shape)\n",
        "print(\"DeepWalk Embeddings: \", len(deepwalk_embeddings), \" nodes\")\n",
        "print(\"node2vec Embeddings: \", len(node2vec_embeddings), \" nodes\")\n",
        "print(\"Struc2Vec (Simulated) Embeddings: \", struc2vec_embeddings.shape)"
      ],
      "metadata": {
        "id": "BBzsH8aj3CZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run clustering and evaluation for each set of embeddings\n",
        "results_laplacian = cluster_and_evaluate(laplacian_embeddings, G)\n",
        "results_hope = cluster_and_evaluate(hope_embeddings, G)\n",
        "results_deepwalk = cluster_and_evaluate(list(deepwalk_embeddings.values()), G)\n",
        "results_node2vec = cluster_and_evaluate(list(node2vec_embeddings.values()), G)\n",
        "results_struc2vec = cluster_and_evaluate(struc2vec_embeddings, G)\n",
        "\n",
        "\n",
        "# Output results\n",
        "print(\"Laplacian Eigenmaps Clustering Results:\", results_laplacian)\n",
        "print(\"HOPE Clustering Results:\", results_hope)\n",
        "print(\"DeepWalk Clustering Results:\", results_deepwalk)\n",
        "print(\"node2vec Clustering Results:\", results_node2vec)\n",
        "print(\"Struc2Vec (Simulated) Clustering Results:\", results_struc2vec)"
      ],
      "metadata": {
        "id": "q_P9UZsNN9ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_cora = to_networkx(data, to_undirected=True)\n",
        "G = G_cora\n",
        "\n",
        "# 1. GCN (Graph Convolution Network)\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 64)\n",
        "        self.conv2 = GCNConv(64, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "gcn = GCN(in_channels=data.x.shape[1], out_channels=64)\n",
        "gcn_embeddings = gcn(data)\n",
        "\n",
        "# 2. GraphSAGE (Graph Sample and Aggregate)\n",
        "class GraphSAGE(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, 64)\n",
        "        self.conv2 = SAGEConv(64, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "graphsage = GraphSAGE(in_channels=data.x.shape[1], out_channels=64)\n",
        "graphsage_embeddings = graphsage(data)\n",
        "\n",
        "# 3. GAT (Graph Attention Network)\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GAT, self).__init__()\n",
        "        self.conv1 = GATConv(in_channels, 64, heads=8)\n",
        "        self.conv2 = GATConv(64 * 8, out_channels, heads=1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "gat = GAT(in_channels=data.x.shape[1], out_channels=64)\n",
        "gat_embeddings = gat(data)\n",
        "\n",
        "# 4. DGI (Deep Graph Infomax)\n",
        "class DGI(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(DGI, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.fc = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "dgi = DGI(in_channels=data.x.shape[1], hidden_channels=64, out_channels=64)\n",
        "dgi_embeddings = dgi(data)\n",
        "\n",
        "# Visualization of embeddings (using PCA for visualization)\n",
        "def plot_embeddings(embeddings, title):\n",
        "    embeddings = embeddings.detach().numpy()\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced_embeddings = pca.fit_transform(embeddings)\n",
        "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1])\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# # Plotting embeddings for each method\n",
        "# plot_embeddings(gcn_embeddings, \"GCN\")\n",
        "# plot_embeddings(graphsage_embeddings, \"GraphSAGE\")\n",
        "# plot_embeddings(gat_embeddings, \"GAT\")\n",
        "# plot_embeddings(dgi_embeddings, \"DGI\")\n",
        "\n",
        "# Output the embeddings for each method\n",
        "print(\"GCN Embeddings: \", gcn_embeddings.shape)\n",
        "print(\"GraphSAGE Embeddings: \", graphsage_embeddings.shape)\n",
        "print(\"GAT Embeddings: \", gat_embeddings.shape)\n",
        "print(\"DGI Embeddings: \", dgi_embeddings.shape)"
      ],
      "metadata": {
        "id": "Mvu-EX61Br53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run clustering and evaluation for each set of embeddings\n",
        "results_gcn = cluster_and_evaluate_tensor(gcn_embeddings, G)\n",
        "results_graphsage = cluster_and_evaluate_tensor(graphsage_embeddings, G)\n",
        "results_gat = cluster_and_evaluate_tensor(gat_embeddings, G)\n",
        "results_dgi = cluster_and_evaluate_tensor(dgi_embeddings, G)\n",
        "\n",
        "# Output results\n",
        "print(\"GCN Clustering Results:\", results_gcn)\n",
        "print(\"GraphSAGE Clustering Results:\", results_graphsage)\n",
        "print(\"GAT Clustering Results:\", results_gat)\n",
        "print(\"DGI Clustering Results:\", results_dgi)"
      ],
      "metadata": {
        "id": "knzDWRGdBr8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert graph to adjacency matrix\n",
        "adj = nx.to_numpy_array(G)\n",
        "\n",
        "# Normalize the adjacency matrix\n",
        "scaler = StandardScaler()\n",
        "adj_scaled = scaler.fit_transform(adj)\n",
        "\n",
        "# Define the Autoencoder model for SDNE\n",
        "input_layer = Input(shape=(adj.shape[1],))\n",
        "encoded = Dense(64, activation='relu')(input_layer)  # Embedding layer\n",
        "decoded = Dense(adj.shape[1], activation='sigmoid')(encoded)\n",
        "\n",
        "autoencoder = Model(input_layer, decoded)\n",
        "autoencoder.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "\n",
        "# Train the Autoencoder\n",
        "autoencoder.fit(adj_scaled, adj_scaled, epochs=50, batch_size=16, shuffle=True, verbose=1)\n",
        "\n",
        "# Get the encoded embeddings (lower-dimensional representation)\n",
        "encoder = Model(input_layer, encoded)\n",
        "sdne_embeddings = encoder.predict(adj_scaled)\n",
        "\n",
        "# Visualize the embeddings using PCA\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(sdne_embeddings)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1])\n",
        "plt.title(\"SDNE Embeddings (Karate Club)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"SDNE Embeddings shape: {sdne_embeddings.shape}\")"
      ],
      "metadata": {
        "id": "Dz8Eq2rCBr-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_sdne = cluster_and_evaluate(sdne_embeddings,G)\n",
        "print(\"SDNE Clustering Results:\", results_sdne)"
      ],
      "metadata": {
        "id": "aX1tag_ABsA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute k-hop adjacency matrices (e.g., for k = 1, 2, 3)\n",
        "adj_k1 = adj\n",
        "adj_k2 = np.matmul(adj, adj)  # k=2 (second-order proximity)\n",
        "adj_k3 = np.matmul(adj_k2, adj)  # k=3 (third-order proximity)\n",
        "\n",
        "# Normalize each k-hop adjacency matrix to stabilize NMF\n",
        "adj_k1 /= adj_k1.max()\n",
        "adj_k2 /= adj_k2.max()\n",
        "adj_k3 /= adj_k3.max()\n",
        "\n",
        "# Stack all k-hop matrices\n",
        "adj_matrices = [adj_k1, adj_k2, adj_k3]\n",
        "combined_adj = np.hstack(adj_matrices)\n",
        "\n",
        "# Apply Non-negative Matrix Factorization (NMF) for embedding\n",
        "nmf = NMF(n_components=64, init='random', random_state=42, max_iter=200)\n",
        "embeddings = nmf.fit_transform(combined_adj)\n",
        "\n",
        "# Visualize the embeddings using PCA\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(embeddings)\n",
        "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=list(G.nodes))\n",
        "plt.title(\"GraRep Embeddings (Karate Club)\")\n",
        "plt.colorbar(label=\"Node Index\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"GraRep Embeddings shape: {embeddings.shape}\")"
      ],
      "metadata": {
        "id": "hHksv7_ZGYLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_GraRep = cluster_and_evaluate(embeddings,G)\n",
        "print(\"GraRep Clustering Results:\", results_GraRep)"
      ],
      "metadata": {
        "id": "KmRm0Z83OhbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert adjacency matrix to torch tensor\n",
        "adj_tensor = torch.tensor(adj, dtype=torch.float32)\n",
        "\n",
        "# Define the Graph Autoencoder model\n",
        "class GraphAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(GraphAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
        "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = F.relu(self.encoder(x))  # Encoding step\n",
        "        return torch.sigmoid(self.decoder(z))  # Decoding step\n",
        "\n",
        "# Initialize model, optimizer and loss function\n",
        "model = GraphAutoencoder(input_dim=adj_tensor.shape[0], hidden_dim=64)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(adj_tensor)\n",
        "    loss = F.binary_cross_entropy(output, adj_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# Get the embeddings (encoded representations)\n",
        "embeddings = model.encoder(adj_tensor).detach().numpy()"
      ],
      "metadata": {
        "id": "6mTod6XKPo0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_G_AE = cluster_and_evaluate(embeddings,G)\n",
        "print(\"Graph Autoencoder Clustering Results:\", results_G_AE)"
      ],
      "metadata": {
        "id": "3x1_a_MCPqhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the VGAE model with an additional decoder layer\n",
        "class VariationalGraphAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim, output_dim):\n",
        "        super(VariationalGraphAutoencoder, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, latent_dim)\n",
        "        self.decoder = nn.Linear(latent_dim, output_dim)  # Decoder layer to reconstruct original features\n",
        "\n",
        "    def encode(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        return self.conv2(x, edge_index)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        z = self.encode(x, edge_index)\n",
        "        return self.decoder(z)  # Reconstruct node features\n",
        "\n",
        "# Initialize model, optimizer and loss function\n",
        "model = VariationalGraphAutoencoder(input_dim=dataset.num_node_features, hidden_dim=64, latent_dim=64, output_dim=dataset.num_node_features)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1000):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    reconstructed_x = model(data.x, data.edge_index)\n",
        "    loss = F.mse_loss(reconstructed_x, data.x)  # Reconstruct node features\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# Get the embeddings (encoded representations)\n",
        "embeddings = model.encode(data.x, data.edge_index).detach().numpy()"
      ],
      "metadata": {
        "id": "wtgDrlKtQcsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_VAGE = cluster_and_evaluate(embeddings,G)\n",
        "print(\"Graph Autoencoder Decoder Clustering Results:\", results_VAGE)"
      ],
      "metadata": {
        "id": "JWTguE5SQedH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Poincaré embedding model (using a custom library)\n",
        "class PoincareEmbedding(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(PoincareEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x)\n",
        "\n",
        "# Example usage for a small graph or tree\n",
        "input_dim = G_cora.number_of_nodes()\n",
        "latent_dim = 64\n",
        "model = PoincareEmbedding(input_dim, latent_dim)\n",
        "\n",
        "# Random input nodes\n",
        "nodes = torch.LongTensor([i for i in range(input_dim)])\n",
        "\n",
        "# Forward pass to get embeddings\n",
        "embeddings = model(nodes)\n",
        "\n",
        "# Apply PCA to reduce embeddings to 2D for visualization\n",
        "pca = PCA(n_components=2)\n",
        "embeddings_2d = pca.fit_transform(embeddings.detach().numpy())\n",
        "\n",
        "# Visualize in 2D\n",
        "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
        "plt.title(\"Poincaré Embeddings (PCA Reduced to 2D)\")\n",
        "plt.show()\n",
        "\n",
        "# Print embeddings shape\n",
        "print(\"Embeddings Shape: \", embeddings.shape)"
      ],
      "metadata": {
        "id": "A6FG2eWGs-SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_Poincare = cluster_and_evaluate_tensor(embeddings,G)\n",
        "print(\"Poincaré Results:\", results_Poincare)"
      ],
      "metadata": {
        "id": "pWbkar0qtaEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bjbNUdB9rWzx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}