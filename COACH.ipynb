{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAKzx0fpqaryLnNl5VlZ4M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vibhuverma17/COACH/blob/main/COACH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost hyperopt\n",
        "!pip install --upgrade scipy"
      ],
      "metadata": {
        "id": "tuJPUdCpwFce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ilck8dKnYfc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import xgboost as xgb\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve, confusion_matrix, classification_report\n",
        "from scipy.stats import binomtest\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define constants\n",
        "num_rows = 15000\n",
        "months = pd.date_range(start=\"2024-01-01\", periods=12, freq='M').strftime('%Y-%m').tolist()\n",
        "age_groups = ['18-24', '25-34', '35-44', '45-54', '55+']\n",
        "genders = ['Male', 'Female', 'Non-binary']\n",
        "education_levels = ['High School', 'Associate\\'s', 'Bachelor\\'s', 'Master\\'s', 'Doctorate']\n",
        "locations = ['Urban', 'Suburban', 'Rural']\n",
        "income_ranges = ['$20,000 - $30,000', '$30,000 - $50,000', '$50,000 - $70,000',\n",
        "                '$70,000 - $90,000', '$90,000 - $110,000']\n",
        "\n",
        "# Generate data\n",
        "data = {\n",
        "    \"Survey_ID\": range(1, num_rows + 1),\n",
        "    \"Survey_Month\": [random.choice(months) for _ in range(num_rows)],\n",
        "    \"Age_Group\": [random.choice(age_groups) for _ in range(num_rows)],\n",
        "    \"Gender\": [random.choice(genders) for _ in range(num_rows)],\n",
        "    \"Education_Level\": [random.choice(education_levels) for _ in range(num_rows)],\n",
        "    \"Location\": [random.choice(locations) for _ in range(num_rows)],\n",
        "    \"Income_Range\": [random.choice(income_ranges) for _ in range(num_rows)],\n",
        "    \"Happiness_Score\": np.random.choice([0, 1], size=num_rows),  # Binary 0 or 1\n",
        "    \"Work_Satisfaction\": np.random.randint(1, 11, size=num_rows),  # 1 to 10\n",
        "    \"Social_Interactions\": np.random.randint(1, 11, size=num_rows),  # 1 to 10\n",
        "    \"Physical_Health\": np.random.randint(1, 11, size=num_rows),  # 1 to 10\n",
        "    \"Mental_Health\": np.random.randint(1, 11, size=num_rows),  # 1 to 10\n",
        "    \"Major_Stressors\": [random.choice(['Job', 'Finances', 'Health', 'Relationships', 'Family', 'Workload']) for _ in range(num_rows)],\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "survey_df = pd.DataFrame(data)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "survey_df.head()\n",
        "\n",
        "# Save to CSV if needed\n",
        "# survey_df.to_csv(\"happiness_survey_data.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Ojo8KSs7nd_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the preprocess function as a pipeline\n",
        "def create_preprocessing_pipeline():\n",
        "    # Define the column transformer\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('cat', OneHotEncoder(drop='first'), ['Age_Group', 'Gender', 'Education_Level', 'Location', 'Income_Range','Major_Stressors']),\n",
        "        ],\n",
        "        remainder='passthrough'  # Keep other columns unchanged\n",
        "    )\n",
        "\n",
        "    # Define the full pipeline\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor)\n",
        "    ])\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "def preprocess_data(data):\n",
        "    # Convert 'Survey_Month' to datetime and extract year and month\n",
        "    data['Survey_Month'] = pd.to_datetime(data['Survey_Month'])\n",
        "    data['Year'] = data['Survey_Month'].dt.year\n",
        "    data['Month'] = data['Survey_Month'].dt.month\n",
        "\n",
        "    # Sort by year and month to ensure correct order\n",
        "    data = data.sort_values(by=['Year', 'Month'])\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "2-S2OcpBnoDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the entire dataset\n",
        "preprocessed_df = preprocess_data(survey_df)\n",
        "\n",
        "# Create a preprocessing pipeline\n",
        "pipeline = create_preprocessing_pipeline()\n",
        "\n",
        "# Separate into features and target variable\n",
        "X_full = preprocessed_df.drop(columns=['Survey_ID', 'Happiness_Score', 'Survey_Month'])\n",
        "y_full = preprocessed_df['Happiness_Score']\n",
        "\n",
        "# Fit the pipeline on the full data\n",
        "X_transformed = pipeline.fit_transform(X_full)\n",
        "\n",
        "# Identify the latest month\n",
        "latest_month = preprocessed_df['Survey_Month'].max()\n",
        "\n",
        "# Separate into training and testing sets\n",
        "train_indices = preprocessed_df['Survey_Month'] < latest_month\n",
        "X_train = X_transformed[train_indices]\n",
        "y_train = y_full[train_indices]\n",
        "\n",
        "# Prepare the test set\n",
        "test_indices = preprocessed_df['Survey_Month'] == latest_month\n",
        "X_test = X_transformed[test_indices]\n",
        "y_test = y_full[test_indices]"
      ],
      "metadata": {
        "id": "8zpJ6Y8yw4dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XGBoostHyperparameterTuner:\n",
        "    def __init__(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        self.model = None\n",
        "        self.best_params = None\n",
        "\n",
        "    def objective(self, params):\n",
        "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        loglosses = []\n",
        "\n",
        "        for train_index, val_index in kf.split(self.X_train):\n",
        "            # Use iloc to ensure positional indexing\n",
        "            X_kf_train, X_kf_val = self.X_train[train_index], self.X_train[val_index]\n",
        "            y_kf_train, y_kf_val = self.y_train.iloc[train_index], self.y_train.iloc[val_index]\n",
        "\n",
        "            dtrain = xgb.DMatrix(X_kf_train, label=y_kf_train)\n",
        "            dval = xgb.DMatrix(X_kf_val, label=y_kf_val)\n",
        "\n",
        "            # Train the model with early stopping\n",
        "            model = xgb.train(params, dtrain, num_boost_round=100,\n",
        "                              evals=[(dval, 'eval')],\n",
        "                              early_stopping_rounds=10,\n",
        "                              verbose_eval=False)\n",
        "\n",
        "            # Predict on validation data and calculate logloss\n",
        "            preds = model.predict(dval)\n",
        "            logloss = -1 * np.mean(np.log((preds - y_kf_val)**2))\n",
        "            loglosses.append(logloss)\n",
        "\n",
        "        # Return mean logloss across all folds\n",
        "        mean_logloss = np.mean(loglosses)\n",
        "        return {'loss': mean_logloss, 'status': STATUS_OK}\n",
        "\n",
        "    def tune_hyperparameters(self, max_evals=50):\n",
        "        space = {\n",
        "            'max_depth': hp.randint('max_depth', 3, 10),\n",
        "            'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
        "            'n_estimators': hp.randint('n_estimators', 50, 200),\n",
        "            'gamma': hp.uniform('gamma', 0, 5),\n",
        "            'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
        "            'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
        "            'objective': 'binary:logistic',\n",
        "            'eval_metric': 'logloss'\n",
        "        }\n",
        "\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=self.objective, space=space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
        "        self.best_params = best\n",
        "        return best\n",
        "\n",
        "    def train_final_model(self):\n",
        "        dtrain = xgb.DMatrix(self.X_train, label=self.y_train)\n",
        "        self.model = xgb.train({**self.best_params, 'objective': 'binary:logistic', 'eval_metric': 'logloss'}, dtrain, num_boost_round=100)\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.model is None:\n",
        "            raise Exception(\"Model has not been trained yet!\")\n",
        "        dval = xgb.DMatrix(X)\n",
        "        return self.model.predict(dval)"
      ],
      "metadata": {
        "id": "IXsdOqsFwrNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and tune the model\n",
        "tuner = XGBoostHyperparameterTuner(X_train, y_train)\n",
        "best_params = tuner.tune_hyperparameters(max_evals=50)\n",
        "print(\"Best hyperparameters:\", best_params)\n",
        "\n",
        "# Train the final model with the best hyperparameters\n",
        "tuner.train_final_model()\n",
        "\n",
        "# Access the trained model\n",
        "model = tuner.model\n",
        "\n",
        "# Make predictions on the validation set\n",
        "predictions = tuner.predict(X_train)"
      ],
      "metadata": {
        "id": "w_YHt0wu9_7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_cutoff(y_true, y_prob):\n",
        "    # Calculate the false positive rate (fpr), true positive rate (tpr), and thresholds\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
        "\n",
        "    # Calculate Youden's J statistic\n",
        "    J = tpr - fpr\n",
        "\n",
        "    # Find the index of the maximum J statistic\n",
        "    best_index = np.argmax(J)\n",
        "    best_threshold = thresholds[best_index]\n",
        "\n",
        "    return best_threshold\n",
        "\n",
        "# Get the predicted probabilities on the training set\n",
        "train_predictions_prob = tuner.predict(X_train)\n",
        "\n",
        "# Find the best threshold using Youden's J statistic\n",
        "best_threshold = find_best_cutoff(y_train, train_predictions_prob)\n",
        "\n",
        "# Convert probabilities to class labels using the best threshold\n",
        "train_predictions = (train_predictions_prob > best_threshold).astype(int)\n",
        "\n",
        "# Print the best threshold\n",
        "print(f\"Best Threshold (using Youden's J statistic): {best_threshold}\")\n",
        "\n",
        "# Get the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_train, train_predictions)\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Get the classification report (includes precision, recall, f1-score)\n",
        "class_report = classification_report(y_train, train_predictions)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "id": "DorcvmJ39dop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variable to store the filtered indices\n",
        "filtered_indices = []\n",
        "\n",
        "# Step 1: Identify non-numeric columns (excluding 'Survey ID')\n",
        "non_numeric_cols = preprocessed_df[preprocessed_df['Survey_Month'] == latest_month].select_dtypes(exclude=['number']).columns.tolist()\n",
        "if 'Survey ID' in non_numeric_cols:\n",
        "    non_numeric_cols.remove('Survey ID')\n",
        "\n",
        "# Step 2: Create widgets for selecting values from non-numeric columns\n",
        "dropdowns = {}\n",
        "for col in non_numeric_cols:\n",
        "    unique_values = preprocessed_df[preprocessed_df['Survey_Month'] == latest_month][col].unique().tolist()\n",
        "    dropdowns[col] = widgets.Dropdown(\n",
        "        options=['All'] + unique_values,\n",
        "        description=col,\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "# Function to apply the filters and return the index values of filtered rows\n",
        "def apply_filters(*args):\n",
        "    global filtered_indices\n",
        "    filtered_df = preprocessed_df[preprocessed_df['Survey_Month'] == latest_month].copy()\n",
        "\n",
        "    # Apply filters\n",
        "    for col, dropdown in dropdowns.items():\n",
        "        if dropdown.value != 'All':\n",
        "            filtered_df = filtered_df[filtered_df[col] == dropdown.value]\n",
        "\n",
        "    # Step 3: Get index values of filtered rows\n",
        "    filtered_indices = filtered_df.index.tolist()\n",
        "\n",
        "    # Display the indices\n",
        "    print(\"Filtered row indices Done\")\n",
        "\n",
        "# Step 4: Create an \"Apply Filters\" button\n",
        "apply_button = widgets.Button(description=\"Apply Filters\")\n",
        "apply_button.on_click(apply_filters)\n",
        "\n",
        "# Display dropdowns and button\n",
        "display(widgets.VBox(list(dropdowns.values()) + [apply_button]))\n"
      ],
      "metadata": {
        "id": "M-WKV6wgXIbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of successes (e.g., heads in a coin flip)\n",
        "successes = np.sum(y_test[\n",
        "    (preprocessed_df['Survey_Month'] == latest_month) &\n",
        "    (preprocessed_df.index.isin(filtered_indices))])\n",
        "\n",
        "# Number of trials (e.g., total number of coin flips)\n",
        "trials = y_test[\n",
        "    (preprocessed_df['Survey_Month'] == latest_month) &\n",
        "    (preprocessed_df.index.isin(filtered_indices))].shape[0]\n",
        "\n",
        "# Hypothesized probability of success (e.g., fair coin: p = 0.5)\n",
        "p = np.mean(tuner.predict(X_transformed[\n",
        "    (preprocessed_df['Survey_Month'] == latest_month) &\n",
        "    (preprocessed_df.index.isin(filtered_indices))]))\n",
        "\n",
        "# Perform the binomial test\n",
        "p_value = binomtest(successes, n=trials, p=p, alternative='two-sided').pvalue\n",
        "\n",
        "print(f'P-value of the test: {p_value}')\n",
        "\n",
        "# Check significance at 0.05 level\n",
        "if p_value < 0.05:\n",
        "    print(\"The result is statistically significant at the 0.05 level and this is an outlier.\")\n",
        "else:\n",
        "    print(\"The result is not statistically significant at the 0.05 level and this is not an outlier.\")"
      ],
      "metadata": {
        "id": "y-pRzXIln6i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of successes (e.g., heads in a coin flip)\n",
        "successes = np.sum(y_test[\n",
        "    (preprocessed_df['Survey_Month'] == latest_month) &\n",
        "    (preprocessed_df.index.isin(filtered_indices))])\n",
        "\n",
        "# Number of trials (e.g., total number of coin flips)\n",
        "trials = y_test[\n",
        "    (preprocessed_df['Survey_Month'] == latest_month) &\n",
        "    (preprocessed_df.index.isin(filtered_indices))].shape[0]\n",
        "\n",
        "# Hypothesized probability of success (e.g., fair coin: p = 0.5)\n",
        "p = np.mean(y_test)\n",
        "\n",
        "# Perform the binomial test\n",
        "p_value = binomtest(successes, n=trials, p=p, alternative='two-sided').pvalue\n",
        "\n",
        "print(f'P-value of the test: {p_value}')\n",
        "\n",
        "# Check significance at 0.05 level\n",
        "if p_value < 0.05:\n",
        "    print(\"The result is statistically significant at the 0.05 level and this is an outlier.\")\n",
        "else:\n",
        "    print(\"The result is not statistically significant at the 0.05 level and this is not an outlier.\")"
      ],
      "metadata": {
        "id": "8pVFxW0DBdfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Identify non-numeric columns (excluding 'Survey ID')\n",
        "non_numeric_cols = preprocessed_df[preprocessed_df['Survey_Month'] == latest_month].select_dtypes(exclude=['number']).columns.tolist()\n",
        "if 'Survey ID' in non_numeric_cols:\n",
        "    non_numeric_cols.remove('Survey ID')\n",
        "\n",
        "# Step 2: Function to get all combinations of selected values for non-numeric columns\n",
        "def get_combinations():\n",
        "    unique_value_lists = []\n",
        "\n",
        "    # Collect the unique values for each non-numeric column\n",
        "    for col in non_numeric_cols:\n",
        "        unique_values = preprocessed_df[preprocessed_df['Survey_Month'] == latest_month][col].unique().tolist()\n",
        "        unique_value_lists.append(['All'] + unique_values)\n",
        "\n",
        "    # Generate all possible combinations of values\n",
        "    return list(itertools.product(*unique_value_lists))\n",
        "\n",
        "# Step 3: Apply a specific combination of filters to the dataset\n",
        "def apply_filter_combination(combo):\n",
        "    filtered_df = preprocessed_df[preprocessed_df['Survey_Month'] == latest_month].copy()\n",
        "\n",
        "    # Apply filters based on the current combination of values\n",
        "    for i, col in enumerate(non_numeric_cols):\n",
        "        if combo[i] != 'All':\n",
        "            filtered_df = filtered_df[filtered_df[col] == combo[i]]\n",
        "\n",
        "    return filtered_df\n",
        "\n",
        "# Step 4: Find valid combinations where filtered rows > 50 and store the results in a DataFrame\n",
        "def find_valid_combinations():\n",
        "    results = []\n",
        "\n",
        "    # Get all possible combinations of values for the non-numeric columns\n",
        "    combinations = get_combinations()\n",
        "\n",
        "    # Loop through each combination and apply the filter\n",
        "    for combo in combinations:\n",
        "        filtered_df = apply_filter_combination(combo)\n",
        "        if len(filtered_df) > 50:  # Check if the number of rows is greater than 50\n",
        "            # Create a row with the combination details\n",
        "            combo_dict = {non_numeric_cols[i]: combo[i] for i in range(len(non_numeric_cols))}\n",
        "            result_row = combo_dict.copy()\n",
        "            result_row[\"Number of Rows\"] = len(filtered_df)\n",
        "            result_row[\"Filtered_Indices\"] = filtered_df.index.tolist()\n",
        "            results.append(result_row)\n",
        "\n",
        "    # Convert the results to a DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "    #print(f\"Number of valid combinations: {len(results_df)}\")\n",
        "    return results_df\n",
        "\n",
        "# Step 5: Run the process\n",
        "valid_combinations_df = find_valid_combinations()"
      ],
      "metadata": {
        "id": "RG6oTIk0Hknv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_outlier(row, y_test, X_transformed, preprocessed_df, tuner, latest_month):\n",
        "    # Number of successes (e.g., sum of y_test for the given filtered index)\n",
        "    successes = np.sum(y_test[\n",
        "        (preprocessed_df['Survey_Month'] == latest_month) &\n",
        "        (preprocessed_df.index.isin(row['Filtered_Indices']))])\n",
        "\n",
        "    # Number of trials is the count of indices for the current row\n",
        "    trials = len(row['Filtered_Indices'])\n",
        "\n",
        "    # Hypothesized probability of success (from the model's predictions)\n",
        "    p = np.mean(tuner.predict(X_transformed[\n",
        "        (preprocessed_df['Survey_Month'] == latest_month) &\n",
        "        (preprocessed_df.index.isin(row['Filtered_Indices']))]))\n",
        "\n",
        "    # Perform the binomial test\n",
        "    p_value = binomtest(successes, n=trials, p=p, alternative='two-sided').pvalue\n",
        "\n",
        "    # Return 1 if statistically significant (outlier), otherwise 0\n",
        "    return 1 if p_value < 0.05 else 0\n",
        "\n",
        "def apply_outlier_check(valid_combinations_df, y_test, X_transformed, preprocessed_df, tuner, latest_month):\n",
        "    valid_combinations_df['Outlier_Status'] = valid_combinations_df.apply(\n",
        "        lambda row: check_outlier(row, y_test, X_transformed, preprocessed_df, tuner, latest_month),\n",
        "        axis=1\n",
        "    )\n",
        "    return valid_combinations_df"
      ],
      "metadata": {
        "id": "cpctCZiLU33V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_outlier_std(row, y_test, X_transformed, preprocessed_df, tuner, latest_month):\n",
        "    # Number of successes (e.g., sum of y_test for the given filtered index)\n",
        "    successes = np.sum(y_test[\n",
        "        (preprocessed_df['Survey_Month'] == latest_month) &\n",
        "        (preprocessed_df.index.isin(row['Filtered_Indices']))])\n",
        "\n",
        "    # Number of trials is the count of indices for the current row\n",
        "    trials = len(row['Filtered_Indices'])\n",
        "\n",
        "    # Hypothesized probability of success (from the model's predictions)\n",
        "    p = np.mean(y_test)\n",
        "\n",
        "    # Perform the binomial test\n",
        "    p_value = binomtest(successes, n=trials, p=p, alternative='two-sided').pvalue\n",
        "\n",
        "    # Return 1 if statistically significant (outlier), otherwise 0\n",
        "    return 1 if p_value < 0.05 else 0\n",
        "\n",
        "def apply_outlier_check_std(valid_combinations_df, y_test, X_transformed, preprocessed_df, tuner, latest_month):\n",
        "    valid_combinations_df['Outlier_Status_std'] = valid_combinations_df.apply(\n",
        "        lambda row: check_outlier_std(row, y_test, X_transformed, preprocessed_df, tuner, latest_month),\n",
        "        axis=1\n",
        "    )\n",
        "    return valid_combinations_df"
      ],
      "metadata": {
        "id": "NxKtT9Jobqkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_df = apply_outlier_check(valid_combinations_df, y_test, X_transformed, preprocessed_df, tuner, latest_month)\n",
        "updated_df = apply_outlier_check_std(valid_combinations_df, y_test, X_transformed, preprocessed_df, tuner, latest_month)"
      ],
      "metadata": {
        "id": "KDafPdpLVDG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_df.head()"
      ],
      "metadata": {
        "id": "ymk9xQOKajw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find rows where Outlier_Status is 1 and Outlier_Status_std is 0\n",
        "only_in_first = updated_df[(updated_df['Outlier_Status'] == 1) & (updated_df['Outlier_Status_std'] == 0)]\n",
        "\n",
        "# Find rows where Outlier_Status is 0 and Outlier_Status_std is 1\n",
        "only_in_second = updated_df[(updated_df['Outlier_Status'] == 0) & (updated_df['Outlier_Status_std'] == 1)]\n",
        "\n",
        "# Find rows where both are 1\n",
        "present_in_both = updated_df[(updated_df['Outlier_Status'] == 1) & (updated_df['Outlier_Status_std'] == 1)]\n",
        "\n",
        "# Find rows where both are 0\n",
        "absent_in_both = updated_df[(updated_df['Outlier_Status'] == 0) & (updated_df['Outlier_Status_std'] == 0)]\n",
        "\n",
        "# Output the results\n",
        "print(\"Rows where Outlier_Status is 1 and Outlier_Status_std is 0:\")\n",
        "print(only_in_first.shape[0])\n",
        "\n",
        "print(\"\\nRows where Outlier_Status is 0 and Outlier_Status_std is 1:\")\n",
        "print(only_in_second.shape[0])\n",
        "\n",
        "print(\"\\nRows where both are 1:\")\n",
        "print(present_in_both.shape[0])\n",
        "\n",
        "print(\"\\nRows where both are 0:\")\n",
        "print(absent_in_both.shape[0])"
      ],
      "metadata": {
        "id": "JLz2uZS7cnvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "-lP0gY1mXcwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define constants\n",
        "num_rows = 15000\n",
        "age_groups = ['18-24', '25-34', '35-44', '45-54', '55+']\n",
        "genders = ['Male', 'Female', 'Non-binary']\n",
        "education_levels = ['High School', 'Associate\\'s', 'Bachelor\\'s', 'Master\\'s', 'Doctorate']\n",
        "locations = ['Urban', 'Suburban', 'Rural']\n",
        "income_ranges = ['$20,000 - $30,000', '$30,000 - $50,000', '$50,000 - $70,000',\n",
        "                '$70,000 - $90,000', '$90,000 - $110,000']\n",
        "\n",
        "# Generate data\n",
        "data = {\n",
        "    \"Survey_ID\": range(1, num_rows + 1),\n",
        "    \"Age_Group\": [random.choice(age_groups) for _ in range(num_rows)],\n",
        "    \"Gender\": [random.choice(genders) for _ in range(num_rows)],\n",
        "    \"Education_Level\": [random.choice(education_levels) for _ in range(num_rows)],\n",
        "    \"Location\": [random.choice(locations) for _ in range(num_rows)],\n",
        "    \"Income_Range\": [random.choice(income_ranges) for _ in range(num_rows)],\n",
        "    \"Happiness_Score\": np.random.choice([0, 1], size=num_rows),  # Binary 0 or 1\n",
        "    \"Work_Satisfaction\": np.random.randint(1, 11, size=num_rows),  # 1 to 10\n",
        "    \"Social_Interactions\": np.random.randint(1, 11, size=num_rows),  # 1 to 10\n",
        "    \"Physical_Health\": np.random.randint(1, 11, size=num_rows),  # 1 to 10\n",
        "    \"Mental_Health\": np.random.randint(1, 11, size=num_rows),  # 1 to 10\n",
        "    \"Major_Stressors\": [random.choice(['Job', 'Finances', 'Health', 'Relationships', 'Family', 'Workload']) for _ in range(num_rows)],\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "survey_df = pd.DataFrame(data)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "survey_df.head()\n",
        "\n",
        "# Save to CSV if needed\n",
        "# survey_df.to_csv(\"happiness_survey_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "5C4G5QhKUCNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_combiner(feature, category):\n",
        "    return str(feature) + \":\" + str(category)\n",
        "\n",
        "\n",
        "# Define the preprocess function as a pipeline\n",
        "def create_preprocessing_pipeline():\n",
        "    # Define the column transformer\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('cat', OneHotEncoder(drop='first',feature_name_combiner=custom_combiner), ['Age_Group', 'Gender', 'Education_Level', 'Location', 'Income_Range','Major_Stressors']),\n",
        "        ],\n",
        "        remainder='passthrough'  # Keep other columns unchanged\n",
        "    )\n",
        "\n",
        "    # Define the full pipeline\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor)\n",
        "    ])\n",
        "\n",
        "    return pipeline\n"
      ],
      "metadata": {
        "id": "EMqZpD2mjZR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the entire dataset\n",
        "preprocessed_df = survey_df\n",
        "\n",
        "# Create a preprocessing pipeline\n",
        "pipeline = create_preprocessing_pipeline()\n",
        "\n",
        "# Separate into features and target variable\n",
        "X_full = preprocessed_df.drop(columns=['Survey_ID', 'Happiness_Score'])\n",
        "y_full = preprocessed_df['Happiness_Score']\n",
        "\n",
        "# Fit the pipeline on the full data\n",
        "X_transformed = pipeline.fit_transform(X_full)"
      ],
      "metadata": {
        "id": "Q9mFhnSYjZUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract one-hot encoded column names\n",
        "onehot_columns = pipeline.named_steps['preprocessor'].transformers_[0][1].get_feature_names_out()\n",
        "\n",
        "# Identify passthrough columns\n",
        "passthrough_columns = [col for col in X_full.columns if col not in ['Age_Group', 'Gender', 'Education_Level', 'Location', 'Income_Range', 'Major_Stressors']]\n",
        "\n",
        "# Combine one-hot encoded columns with passthrough columns\n",
        "final_columns = list(onehot_columns) + passthrough_columns\n",
        "\n",
        "# Convert the transformed array to a DataFrame and apply the column names\n",
        "X_transformed_df = pd.DataFrame(X_transformed, columns=final_columns)\n",
        "\n",
        "# Display the first few rows of the transformed DataFrame\n",
        "X_transformed_df.head()"
      ],
      "metadata": {
        "id": "8ltkdUtyqi6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variable to store the filtered indices\n",
        "filtered_indices = []\n",
        "\n",
        "# Step 1: Identify non-numeric columns (excluding 'Survey ID')\n",
        "non_numeric_cols = preprocessed_df.select_dtypes(exclude=['number']).columns.tolist()\n",
        "if 'Survey ID' in non_numeric_cols:\n",
        "    non_numeric_cols.remove('Survey ID')\n",
        "\n",
        "# Step 2: Create widgets for selecting values from non-numeric columns\n",
        "dropdowns = {}\n",
        "for col in non_numeric_cols:\n",
        "    unique_values = preprocessed_df[col].unique().tolist()\n",
        "    dropdowns[col] = widgets.Dropdown(\n",
        "        options=['All'] + unique_values,\n",
        "        description=col,\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "\n",
        "all_cols = []\n",
        "# Function to apply the filters and return the index values of filtered rows\n",
        "def apply_filters(*args):\n",
        "    global filtered_indices\n",
        "    filtered_df = preprocessed_df.copy()\n",
        "\n",
        "    # Apply filters\n",
        "    for col, dropdown in dropdowns.items():\n",
        "        if dropdown.value != 'All':\n",
        "            filtered_df = filtered_df[filtered_df[col] == dropdown.value]\n",
        "            all_cols.append(f'{col}:{dropdown.value}')\n",
        "\n",
        "    # Step 3: Get index values of filtered rows\n",
        "    filtered_indices = filtered_df.index.tolist()\n",
        "\n",
        "    # Display the indices\n",
        "    print(\"Filtered row indices Done\")\n",
        "\n",
        "# Step 4: Create an \"Apply Filters\" button\n",
        "apply_button = widgets.Button(description=\"Apply Filters\")\n",
        "apply_button.on_click(apply_filters)\n",
        "\n",
        "# Display dropdowns and button\n",
        "display(widgets.VBox(list(dropdowns.values()) + [apply_button]))"
      ],
      "metadata": {
        "id": "070KJFxwjZZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,len(all_cols)):\n",
        "  print(str(all_cols[i]))\n"
      ],
      "metadata": {
        "id": "9X9JaizVsbwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_cols = [x.split(':')[0] for x in all_cols ]\n",
        "filtered_columns = [col for col in X_transformed_df.columns if any(keyword in col for keyword in all_cols)]\n",
        "X_transformed_df[filtered_columns] = np.nan"
      ],
      "metadata": {
        "id": "vRq41Wf14jP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_indices = preprocessed_df.index.difference(filtered_indices)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train = X_transformed[train_indices]\n",
        "y_train = preprocessed_df.loc[train_indices]['Happiness_Score']\n",
        "X_test = X_transformed[filtered_indices]\n",
        "y_test = preprocessed_df.loc[filtered_indices]['Happiness_Score']"
      ],
      "metadata": {
        "id": "4tvAZms2l1ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XGBoostHyperparameterTuner:\n",
        "    def __init__(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        self.model = None\n",
        "        self.best_params = None\n",
        "\n",
        "    def objective(self, params):\n",
        "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        loglosses = []\n",
        "\n",
        "        for train_index, val_index in kf.split(self.X_train):\n",
        "            # Use iloc to ensure positional indexing\n",
        "            X_kf_train, X_kf_val = self.X_train[train_index], self.X_train[val_index]\n",
        "            y_kf_train, y_kf_val = self.y_train.iloc[train_index], self.y_train.iloc[val_index]\n",
        "\n",
        "            dtrain = xgb.DMatrix(X_kf_train, label=y_kf_train)\n",
        "            dval = xgb.DMatrix(X_kf_val, label=y_kf_val)\n",
        "\n",
        "            # Train the model with early stopping\n",
        "            model = xgb.train(params, dtrain, num_boost_round=100,\n",
        "                              evals=[(dval, 'eval')],\n",
        "                              early_stopping_rounds=10,\n",
        "                              verbose_eval=False)\n",
        "\n",
        "            # Predict on validation data and calculate logloss\n",
        "            preds = model.predict(dval)\n",
        "            logloss = -1 * np.mean(np.log((preds - y_kf_val)**2))\n",
        "            loglosses.append(logloss)\n",
        "\n",
        "        # Return mean logloss across all folds\n",
        "        mean_logloss = np.mean(loglosses)\n",
        "        return {'loss': mean_logloss, 'status': STATUS_OK}\n",
        "\n",
        "    def tune_hyperparameters(self, max_evals=50):\n",
        "        space = {\n",
        "            'max_depth': hp.randint('max_depth', 3, 10),\n",
        "            'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
        "            'n_estimators': hp.randint('n_estimators', 50, 200),\n",
        "            'gamma': hp.uniform('gamma', 0, 5),\n",
        "            'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
        "            'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
        "            'objective': 'binary:logistic',\n",
        "            'eval_metric': 'logloss'\n",
        "        }\n",
        "\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=self.objective, space=space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
        "        self.best_params = best\n",
        "        return best\n",
        "\n",
        "    def train_final_model(self):\n",
        "        dtrain = xgb.DMatrix(self.X_train, label=self.y_train)\n",
        "        self.model = xgb.train({**self.best_params, 'objective': 'binary:logistic', 'eval_metric': 'logloss'}, dtrain, num_boost_round=100)\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.model is None:\n",
        "            raise Exception(\"Model has not been trained yet!\")\n",
        "        dval = xgb.DMatrix(X)\n",
        "        return self.model.predict(dval)"
      ],
      "metadata": {
        "id": "YqeirD3xmIMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if (len(train_indices) < len(filtered_indices)) | (len(train_indices) < 20*len(X_transformed_df.columns)):\n",
        "  print(\"sample too small to train\")\n",
        "else:\n",
        "  # Initialize and tune the model\n",
        "  tuner = XGBoostHyperparameterTuner(X_train, y_train)\n",
        "  best_params = tuner.tune_hyperparameters(max_evals=50)\n",
        "  print(\"Best hyperparameters:\", best_params)\n",
        "\n",
        "  # Train the final model with the best hyperparameters\n",
        "  tuner.train_final_model()\n",
        "\n",
        "  # Access the trained model\n",
        "  model = tuner.model\n",
        "\n",
        "  # Make predictions on the validation set\n",
        "  predictions = tuner.predict(X_train)"
      ],
      "metadata": {
        "id": "t3hx5TDQoEDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if (len(train_indices) < len(filtered_indices)) | (len(train_indices) < 20*len(X_transformed_df.columns)):\n",
        "  print(\"sample too small to train\")\n",
        "else:\n",
        "  # Number of successes (e.g., heads in a coin flip)\n",
        "  successes = np.sum(y_test)\n",
        "\n",
        "  # Number of trials (e.g., total number of coin flips)\n",
        "  trials = y_test.shape[0]\n",
        "\n",
        "  # Hypothesized probability of success (e.g., fair coin: p = 0.5)\n",
        "  p = np.mean(tuner.predict(X_test))\n",
        "\n",
        "  # Perform the binomial test\n",
        "  p_value = binomtest(successes, n=trials, p=p, alternative='two-sided').pvalue\n",
        "\n",
        "  print(f'P-value of the test: {p_value}')\n",
        "\n",
        "  # Check significance at 0.05 level\n",
        "  if p_value < 0.05:\n",
        "      print(\"The result is statistically significant at the 0.05 level and this is an outlier.\")\n",
        "  else:\n",
        "      print(\"The result is not statistically significant at the 0.05 level and this is not an outlier.\")"
      ],
      "metadata": {
        "id": "WpUolPRjoWOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if (len(train_indices) < len(filtered_indices)) | (len(train_indices) < 20*len(X_transformed_df.columns)):\n",
        "  print(\"sample too small to train\")\n",
        "else:\n",
        "  # Number of successes (e.g., heads in a coin flip)\n",
        "  successes = np.sum(y_test)\n",
        "\n",
        "  # Number of trials (e.g., total number of coin flips)\n",
        "  trials = y_test.shape[0]\n",
        "\n",
        "\n",
        "  # Hypothesized probability of success (e.g., fair coin: p = 0.5)\n",
        "  p = np.mean(y_train)\n",
        "\n",
        "  # Perform the binomial test\n",
        "  p_value = binomtest(successes, n=trials, p=p, alternative='two-sided').pvalue\n",
        "\n",
        "  print(f'P-value of the test: {p_value}')\n",
        "\n",
        "  # Check significance at 0.05 level\n",
        "  if p_value < 0.05:\n",
        "      print(\"The result is statistically significant at the 0.05 level and this is an outlier.\")\n",
        "  else:\n",
        "      print(\"The result is not statistically significant at the 0.05 level and this is not an outlier.\")"
      ],
      "metadata": {
        "id": "T8EVpKojCf3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ckstk2WcEgj9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}