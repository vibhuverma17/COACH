{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMNsefQbmtY1mTs/S76R9J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vibhuverma17/COACH/blob/main/Base_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aad7i3jF5Rcf"
      },
      "outputs": [],
      "source": [
        "!pip install umap-learn hdbscan gower kmodes XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Standard Libraries\n",
        "# ===============================\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import ast\n",
        "\n",
        "# ===============================\n",
        "# Visualization Libraries\n",
        "# ===============================\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "\n",
        "# ===============================\n",
        "# Scikit-learn Components\n",
        "# ===============================\n",
        "# Data Splitting and Preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Models\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, RandomForestRegressor,\n",
        "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
        "    IsolationForest\n",
        ")\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (\n",
        "    f1_score, roc_auc_score, accuracy_score, recall_score, precision_score,\n",
        "    mean_absolute_error, mean_squared_error, r2_score,\n",
        "    mean_absolute_percentage_error, roc_curve\n",
        ")\n",
        "\n",
        "# Pairwise distances\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "\n",
        "# ===============================\n",
        "# External Libraries\n",
        "# ===============================\n",
        "import xgboost as xgb  # XGBoost library\n",
        "from scipy.stats import ks_2samp, entropy  # Statistical tests\n",
        "from kmodes.kprototypes import KPrototypes  # Clustering\n",
        "import umap  # Dimensionality reduction\n",
        "import hdbscan  # Density-based clustering\n",
        "import gower  # Gower similarity for mixed data types\n",
        "\n",
        "# ===============================\n",
        "# Configure Warnings\n",
        "# ===============================\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "ov6EwiM05WKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DIM REDUCTION AND GRID SEARCH"
      ],
      "metadata": {
        "id": "uBV8SEqcwY-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StratifiedSamplingUMAP:\n",
        "    def __init__(self, n_neighbors=15, min_dist=0.1, n_components=2, n_grids=10, sample_percentage=0.1):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.min_dist = min_dist\n",
        "        self.n_components = n_components\n",
        "        self.n_grids = n_grids\n",
        "        self.sample_percentage = sample_percentage\n",
        "        self.sample_indices = None  # To store sampled indices\n",
        "\n",
        "    def check_categorical(self, data):\n",
        "        return data.select_dtypes(include=['object']).shape[1] > 0\n",
        "\n",
        "    def fit_transform(self, data):\n",
        "        if self.check_categorical(data):\n",
        "            print(\"Categorical variables detected. Using 'dice' metric for UMAP.\")\n",
        "            categorical_data = data.select_dtypes(include=['object'])\n",
        "            encoder = OneHotEncoder(sparse_output=False)\n",
        "            categorical_encoded = encoder.fit_transform(categorical_data)\n",
        "            continuous_data = data.select_dtypes(exclude=['object'])\n",
        "            combined_data = np.hstack([categorical_encoded, continuous_data])\n",
        "            metric = 'dice'\n",
        "        else:\n",
        "            print(\"No categorical variables detected. Using 'euclidean' metric for UMAP.\")\n",
        "            combined_data = data\n",
        "            metric = 'euclidean'\n",
        "\n",
        "        umap_model = umap.UMAP(n_neighbors=self.n_neighbors, min_dist=self.min_dist,\n",
        "                               n_components=self.n_components, metric=metric)\n",
        "        return umap_model.fit_transform(combined_data)\n",
        "\n",
        "    def stratified_sampling(self, embedding):\n",
        "        min_x, min_y = np.min(embedding[:, :2], axis=0)\n",
        "        max_x, max_y = np.max(embedding[:, :2], axis=0)\n",
        "\n",
        "        x_bins = np.linspace(min_x, max_x, self.n_grids)\n",
        "        y_bins = np.linspace(min_y, max_y, self.n_grids)\n",
        "\n",
        "        if self.n_components == 3:\n",
        "            min_z = np.min(embedding[:, 2])\n",
        "            max_z = np.max(embedding[:, 2])\n",
        "            z_bins = np.linspace(min_z, max_z, self.n_grids)\n",
        "            grid_counts = np.zeros((self.n_grids, self.n_grids, self.n_grids))\n",
        "        else:\n",
        "            grid_counts = np.zeros((self.n_grids, self.n_grids))\n",
        "\n",
        "        for point in embedding:\n",
        "            x_idx = np.digitize(point[0], x_bins) - 1\n",
        "            y_idx = np.digitize(point[1], y_bins) - 1\n",
        "\n",
        "            if self.n_components == 3:\n",
        "                z_idx = np.digitize(point[2], z_bins) - 1\n",
        "                grid_counts[x_idx, y_idx, z_idx] += 1\n",
        "            else:\n",
        "                grid_counts[x_idx, y_idx] += 1\n",
        "\n",
        "        grid_probs = grid_counts / np.sum(grid_counts)\n",
        "        grid_probs_flat = grid_probs.flatten()\n",
        "\n",
        "        # Sample indices based on the probability distribution\n",
        "        sampled_indices = np.random.choice(len(embedding), size=int(len(embedding) * self.sample_percentage), replace=False)\n",
        "\n",
        "        # Store the sampled indices for later retrieval\n",
        "        self.sample_indices = sampled_indices\n",
        "\n",
        "        # Extract the corresponding samples from the embedding\n",
        "        sampled_embedding = embedding[sampled_indices]\n",
        "\n",
        "        return sampled_embedding\n",
        "\n",
        "    def get_sample_indices(self):\n",
        "        \"\"\"\n",
        "        Return the indices of the sampled data points in the original dataset.\n",
        "        \"\"\"\n",
        "        if self.sample_indices is None:\n",
        "            raise ValueError(\"No samples have been selected. Please run stratified_sampling first.\")\n",
        "        return self.sample_indices\n",
        "\n",
        "    def plot(self, embedding):\n",
        "        if self.n_components == 2:\n",
        "            plt.scatter(embedding[:, 0], embedding[:, 1], c='blue', marker='o')\n",
        "            plt.title('UMAP Projection (2D)')\n",
        "            plt.xlabel('UMAP 1')\n",
        "            plt.ylabel('UMAP 2')\n",
        "            plt.show()\n",
        "\n",
        "        elif self.n_components == 3:\n",
        "            fig = plt.figure()\n",
        "            ax = fig.add_subplot(111, projection='3d')\n",
        "            ax.scatter(embedding[:, 0], embedding[:, 1], embedding[:, 2], c='blue', marker='o')\n",
        "            ax.set_title('UMAP Projection (3D)')\n",
        "            ax.set_xlabel('UMAP 1')\n",
        "            ax.set_ylabel('UMAP 2')\n",
        "            ax.set_zlabel('UMAP 3')\n",
        "            plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "XQsa-dNvH2vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of StratifiedSamplingUMAP with sample indices retrieval\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define number of samples, continuous features, and categorical features\n",
        "num_samples = 10000\n",
        "num_continuous_features = 990\n",
        "num_categorical_features = 10\n",
        "\n",
        "# Generate continuous features\n",
        "continuous_data = np.random.randn(num_samples, num_continuous_features)\n",
        "\n",
        "# Generate categorical features (with categories 'A', 'B', 'C')\n",
        "categorical_data = np.random.choice(['A', 'B', 'C'], size=(num_samples, num_categorical_features))\n",
        "\n",
        "# Convert continuous data and categorical data to DataFrames\n",
        "continuous_df = pd.DataFrame(continuous_data, columns=[f\"cont_{i+1}\" for i in range(num_continuous_features)])\n",
        "categorical_df = pd.DataFrame(categorical_data, columns=[f\"cat_{i+1}\" for i in range(num_categorical_features)])\n",
        "\n",
        "# Concatenate continuous and categorical DataFrames along columns to form the complete dataset\n",
        "data = pd.concat([continuous_df, categorical_df], axis=1)\n",
        "\n",
        "# Initialize and use the StratifiedSamplingUMAP class\n",
        "stratified_sampler = StratifiedSamplingUMAP(n_neighbors=15, min_dist=0.1, n_components=3, n_grids=10, sample_percentage=0.1)\n",
        "\n",
        "# Fit and transform the data with UMAP\n",
        "embedding = stratified_sampler.fit_transform(data)\n",
        "\n",
        "# Apply stratified sampling\n",
        "sampled_embedding = stratified_sampler.stratified_sampling(embedding)\n",
        "\n",
        "# Retrieve indices of the sampled data points in the original dataset\n",
        "sample_indices = stratified_sampler.get_sample_indices()\n",
        "print(\"Sampled Data:\", len(sample_indices),\"Data:\",len(data))\n",
        "\n",
        "# Filter the original dataset using these indices\n",
        "sampled_data = data.iloc[sample_indices]\n",
        "\n",
        "# Plot the result\n",
        "stratified_sampler.plot(sampled_embedding)"
      ],
      "metadata": {
        "id": "XuPOtZp3Qyj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stratified_sampler.plot(embedding)"
      ],
      "metadata": {
        "id": "r5CmWo6SQ3h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### HDBSCAN - CLUSTERING BASED SAMPLING"
      ],
      "metadata": {
        "id": "Eao-WLtwwS_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClusterSampler:\n",
        "    def __init__(self, data, sampling_percent=10, **hdbscan_params):\n",
        "        \"\"\"\n",
        "        Initialize the ClusterSampler class.\n",
        "\n",
        "        Parameters:\n",
        "        - data: DataFrame containing the dataset\n",
        "        - sampling_percent: Percentage of points to sample from each cluster (0-100)\n",
        "        - **hdbscan_params: Additional parameters to pass to HDBSCAN\n",
        "        \"\"\"\n",
        "        self.data = self._convert_data_types(data)\n",
        "        self.sampling_percent = sampling_percent\n",
        "        self.is_categorical = self._detect_categorical(data)\n",
        "        self.cluster_labels = None\n",
        "        self.hdbscan_params = hdbscan_params\n",
        "\n",
        "    def _convert_data_types(self, data):\n",
        "        \"\"\"Ensure continuous columns are float64 and categorical columns are object.\"\"\"\n",
        "        continuous_cols = data.select_dtypes(include=['float', 'int']).columns\n",
        "        data[continuous_cols] = data[continuous_cols].astype(np.float64)\n",
        "\n",
        "        categorical_cols = data.select_dtypes(include=['object', 'category']).columns\n",
        "        data[categorical_cols] = data[categorical_cols].astype('object')\n",
        "\n",
        "        return data\n",
        "\n",
        "    def _detect_categorical(self, data):\n",
        "        \"\"\"Detect if the dataset contains categorical features.\"\"\"\n",
        "        return data.select_dtypes(include=['object', 'category']).shape[1] > 0\n",
        "\n",
        "    def _compute_distance_matrix(self):\n",
        "        \"\"\"Compute the distance matrix based on the data type.\"\"\"\n",
        "        if self.is_categorical:\n",
        "            gower_matrix = gower.gower_matrix(self.data)\n",
        "            return gower_matrix.astype(np.float64)\n",
        "        else:\n",
        "            return pairwise_distances(self.data, metric='euclidean')\n",
        "\n",
        "    def fit_clusters(self):\n",
        "        \"\"\"Fit HDBSCAN on the dataset with appropriate distance metric.\"\"\"\n",
        "        distance_matrix = self._compute_distance_matrix()\n",
        "        clusterer = hdbscan.HDBSCAN(metric='precomputed' if self.is_categorical else 'euclidean', **self.hdbscan_params)\n",
        "        self.cluster_labels = clusterer.fit_predict(distance_matrix)\n",
        "\n",
        "    def sample_points(self):\n",
        "        \"\"\"Sample a representative subset of points from each cluster, including noise points as a separate cluster.\"\"\"\n",
        "        if self.cluster_labels is None:\n",
        "            raise ValueError(\"Clusters have not been computed. Call fit_clusters() first.\")\n",
        "\n",
        "        data_with_labels = self.data.copy()\n",
        "        data_with_labels['cluster'] = self.cluster_labels\n",
        "\n",
        "        sampled_indices = []\n",
        "        unique_labels = np.unique(self.cluster_labels)\n",
        "\n",
        "        for cluster_label in unique_labels:\n",
        "            cluster_indices = data_with_labels[data_with_labels['cluster'] == cluster_label].index\n",
        "            sample_size = max(1, int(len(cluster_indices) * (self.sampling_percent / 100)))\n",
        "\n",
        "            # Avoid sampling more points than available\n",
        "            if len(cluster_indices) < sample_size:\n",
        "                print(f\"Cluster {cluster_label} has only {len(cluster_indices)} points, sampling {len(cluster_indices)}.\")\n",
        "            sampled_indices.extend(np.random.choice(cluster_indices, min(sample_size, len(cluster_indices)), replace=False))\n",
        "\n",
        "        return sampled_indices\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Execute the full sampling pipeline: cluster, then sample.\"\"\"\n",
        "        self.fit_clusters()\n",
        "        return self.sample_points()"
      ],
      "metadata": {
        "id": "emjvmMeDMmhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate continuous and categorical data\n",
        "num_samples = 1000\n",
        "num_continuous_features = 990\n",
        "num_categorical_features = 10\n",
        "continuous_data = np.random.randn(num_samples, num_continuous_features)\n",
        "categorical_data = np.random.choice(['A', 'B', 'C'], size=(num_samples, num_categorical_features))\n",
        "\n",
        "# Convert data to DataFrames\n",
        "continuous_df = pd.DataFrame(continuous_data, columns=[f\"cont_{i+1}\" for i in range(num_continuous_features)])\n",
        "categorical_df = pd.DataFrame(categorical_data, columns=[f\"cat_{i+1}\" for i in range(num_categorical_features)])\n",
        "\n",
        "# Concatenate to form complete dataset\n",
        "data = pd.concat([continuous_df, categorical_df], axis=1)\n",
        "\n",
        "# Instantiate and run sampler\n",
        "sampler = ClusterSampler(data, sampling_percent=10, min_cluster_size=5, min_samples=1)\n",
        "sampled_indices = sampler.run()\n",
        "sampled_data = data.loc[sampled_indices]"
      ],
      "metadata": {
        "id": "PPP0qS4f4qXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sampled Data:\", len(sample_indices),\"Data:\",len(data))"
      ],
      "metadata": {
        "id": "sPVeZFSmxdD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ISOLATION FOREST AND KS STATISTIC SAMPLER"
      ],
      "metadata": {
        "id": "xGWSKfBWxJNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnomalySampler:\n",
        "    def __init__(self, X, sample_weight=None):\n",
        "        \"\"\"\n",
        "        Initialize the AnomalySampler class.\n",
        "\n",
        "        Parameters:\n",
        "        - X: DataFrame containing the dataset\n",
        "        - sample_weight: Optional sample weights for the Isolation Forest\n",
        "        \"\"\"\n",
        "        self.original_data = X\n",
        "        self.X = self._one_hot_encode(X)\n",
        "        self.sample_weight = sample_weight\n",
        "        self.preds = self.isolation_forest(self.X, sample_weight)\n",
        "\n",
        "    def _one_hot_encode(self, data):\n",
        "        \"\"\"\n",
        "        Apply one-hot encoding to categorical columns.\n",
        "\n",
        "        Parameters:\n",
        "        - data: DataFrame containing the dataset\n",
        "\n",
        "        Returns:\n",
        "        - One-hot encoded DataFrame\n",
        "        \"\"\"\n",
        "        return pd.get_dummies(data, drop_first=True)  # drop_first to avoid multicollinearity, if relevant\n",
        "\n",
        "    @staticmethod\n",
        "    def isolation_forest(X, sample_weight=None):\n",
        "        \"\"\"\n",
        "        Fits an Isolation Forest to the dataset and assigns an anomaly score to each sample.\n",
        "\n",
        "        Parameters:\n",
        "        - X: DataFrame or array-like containing the dataset\n",
        "        - sample_weight: Optional sample weights for the Isolation Forest\n",
        "\n",
        "        Returns:\n",
        "        - preds: Anomaly scores for each sample\n",
        "        \"\"\"\n",
        "        clf = IsolationForest().fit(X, sample_weight=sample_weight)\n",
        "        preds = clf.score_samples(X)\n",
        "        return preds\n",
        "\n",
        "    @staticmethod\n",
        "    def get_5_percent(num):\n",
        "        \"\"\"Calculate 5% of a given number.\"\"\"\n",
        "        return round(5 / 100 * num)\n",
        "\n",
        "    def get_5_percent_splits(self, length):\n",
        "        \"\"\"Splits a given length into 5% intervals.\"\"\"\n",
        "        five_percent = self.get_5_percent(length)\n",
        "        return np.arange(five_percent, length, five_percent)\n",
        "\n",
        "    def find_sample_indices(self):\n",
        "        \"\"\"\n",
        "        Finds a sample by comparing the distribution of anomaly scores between the sample\n",
        "        and the original distribution using the KS-test. Starts with a 5% sample, increasing\n",
        "        by 5% increments until a significant sample (p-value > 0.95) is found or a limit is reached.\n",
        "\n",
        "        Returns:\n",
        "        - List of indices representing the sample in the original dataset\n",
        "        \"\"\"\n",
        "        size_splits = self.get_5_percent_splits(len(self.X))\n",
        "        element = 1\n",
        "        iteration = 0\n",
        "\n",
        "        while element < len(size_splits):\n",
        "            sample_size = size_splits[element]\n",
        "            sample_indices = np.random.choice(np.arange(self.preds.size), size=sample_size, replace=False)\n",
        "            sample = np.take(self.preds, sample_indices)\n",
        "\n",
        "            # Check if KS test p-value indicates similar distributions\n",
        "            if ks_2samp(self.preds, sample).pvalue > 0.95:\n",
        "                return sample_indices  # Return indices from the original dataset\n",
        "\n",
        "            iteration += 1\n",
        "            if iteration >= 20:\n",
        "                element += 1\n",
        "                iteration = 0\n",
        "\n",
        "        # If no suitable sample is found, return the last attempted sample indices\n",
        "        return sample_indices\n",
        "\n"
      ],
      "metadata": {
        "id": "t4BSBb9W-9kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate continuous and categorical data\n",
        "num_samples = 1000\n",
        "num_continuous_features = 990\n",
        "num_categorical_features = 10\n",
        "continuous_data = np.random.randn(num_samples, num_continuous_features)\n",
        "categorical_data = np.random.choice(['A', 'B', 'C'], size=(num_samples, num_categorical_features))\n",
        "\n",
        "# Convert data to DataFrames\n",
        "continuous_df = pd.DataFrame(continuous_data, columns=[f\"cont_{i+1}\" for i in range(num_continuous_features)])\n",
        "categorical_df = pd.DataFrame(categorical_data, columns=[f\"cat_{i+1}\" for i in range(num_categorical_features)])\n",
        "\n",
        "# Concatenate to form complete dataset\n",
        "data = pd.concat([continuous_df, categorical_df], axis=1)\n",
        "\n",
        "# Initialize AnomalySampler\n",
        "sampler = AnomalySampler(data)\n",
        "sample_indices = sampler.find_sample_indices()\n",
        "sampled_data = data.iloc[sample_indices]  # Retrieve the sampled data based on indices"
      ],
      "metadata": {
        "id": "mp7yTgAPm9RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sampled Data:\", len(sample_indices),\"Data:\",len(data))"
      ],
      "metadata": {
        "id": "ICoNA_oFxfGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ENTROPY SAMPLER"
      ],
      "metadata": {
        "id": "hRaDp-XhxRho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EntropySampler:\n",
        "    def __init__(self, data, sampling_percent=10, bins=10):\n",
        "        \"\"\"\n",
        "        Initialize the EntropySampler class.\n",
        "\n",
        "        Parameters:\n",
        "        - data: DataFrame containing the dataset\n",
        "        - sampling_percent: Percentage of points to sample based on entropy (0-100)\n",
        "        - bins: Number of bins to use for continuous data entropy calculation\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.sampling_percent = sampling_percent\n",
        "        self.bins = bins\n",
        "        self.entropy_scores = None\n",
        "\n",
        "    def _calculate_entropy(self):\n",
        "        \"\"\"\n",
        "        Calculate the entropy for each feature and aggregate entropy per data point.\n",
        "\n",
        "        Returns:\n",
        "        - entropy_scores: Array of entropy scores for each data point\n",
        "        \"\"\"\n",
        "        entropy_scores = np.zeros(len(self.data))\n",
        "\n",
        "        for col in self.data.columns:\n",
        "            if pd.api.types.is_numeric_dtype(self.data[col]):\n",
        "                # For continuous data, bin it and calculate entropy over the bins\n",
        "                counts, _ = np.histogram(self.data[col], bins=self.bins)\n",
        "                feature_entropy = entropy(counts + 1e-10)  # Add small value to avoid log(0)\n",
        "                feature_contributions = np.digitize(self.data[col], bins=np.histogram_bin_edges(self.data[col], bins=self.bins))\n",
        "            else:\n",
        "                # For categorical data, calculate entropy over unique values\n",
        "                counts = self.data[col].value_counts().values\n",
        "                feature_entropy = entropy(counts + 1e-10)\n",
        "                feature_contributions = self.data[col].map(self.data[col].value_counts(normalize=True)).values\n",
        "\n",
        "            # Accumulate entropy scores based on the feature contributions for each data point\n",
        "            entropy_scores += feature_contributions * feature_entropy\n",
        "\n",
        "        self.entropy_scores = entropy_scores\n",
        "\n",
        "    def sample_indices(self):\n",
        "        \"\"\"\n",
        "        Get indices of points with the highest entropy scores.\n",
        "\n",
        "        Returns:\n",
        "        - List of indices for sampled points based on entropy scores\n",
        "        \"\"\"\n",
        "        if self.entropy_scores is None:\n",
        "            self._calculate_entropy()\n",
        "\n",
        "        # Determine the number of points to sample\n",
        "        num_samples = max(1, int(len(self.data) * (self.sampling_percent / 100)))\n",
        "\n",
        "        # Get the indices of the top entropy scores\n",
        "        top_indices = np.argsort(self.entropy_scores)[-num_samples:]\n",
        "\n",
        "        return top_indices\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Execute the full entropy-based sampling process.\n",
        "\n",
        "        Returns:\n",
        "        - List of indices for sampled points based on entropy scores\n",
        "        \"\"\"\n",
        "        self._calculate_entropy()\n",
        "        return self.sample_indices()"
      ],
      "metadata": {
        "id": "3a80f3-ir6gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate continuous and categorical data\n",
        "num_samples = 1000\n",
        "num_continuous_features = 990\n",
        "num_categorical_features = 10\n",
        "continuous_data = np.random.randn(num_samples, num_continuous_features)\n",
        "categorical_data = np.random.choice(['A', 'B', 'C'], size=(num_samples, num_categorical_features))\n",
        "\n",
        "# Convert data to DataFrames\n",
        "continuous_df = pd.DataFrame(continuous_data, columns=[f\"cont_{i+1}\" for i in range(num_continuous_features)])\n",
        "categorical_df = pd.DataFrame(categorical_data, columns=[f\"cat_{i+1}\" for i in range(num_categorical_features)])\n",
        "\n",
        "# Concatenate to form complete dataset\n",
        "data = pd.concat([continuous_df, categorical_df], axis=1)\n",
        "\n",
        "# Initialize EntropySampler\n",
        "sampler = EntropySampler(data)\n",
        "sample_indices = sampler.run()  # Get indices of sampled points\n",
        "sampled_data = data.iloc[sample_indices]  # Retrieve sampled data from the original DataFrame"
      ],
      "metadata": {
        "id": "OkRMBEXitrwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sampled Data:\", len(sample_indices),\"Data:\",len(data))"
      ],
      "metadata": {
        "id": "SG-e9BmOtwcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DISTANCE BASED SAMPLER"
      ],
      "metadata": {
        "id": "r8XnIZ2O33UO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DistanceBasedSampler:\n",
        "    def __init__(self, data, k=5, sampling_percent=10):\n",
        "        \"\"\"\n",
        "        Initialize the DistanceBasedSampler class for distance-based sampling with mixed data.\n",
        "\n",
        "        Parameters:\n",
        "        - data: DataFrame containing the dataset\n",
        "        - k: Number of clusters (or core-set size) to select\n",
        "        - sampling_percent: Percentage of points to sample (for selecting a subset of the cluster centers)\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.k = k\n",
        "        self.sampling_percent = sampling_percent\n",
        "        self.cluster_labels = None\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        \"\"\"\n",
        "        Prepare the data by converting categorical columns to string types for k-prototypes.\n",
        "        \"\"\"\n",
        "        self.data = self.data.apply(lambda col: col.astype(str) if col.dtype == 'object' else col)\n",
        "        return self.data\n",
        "\n",
        "    def fit_clusters(self):\n",
        "        \"\"\"\n",
        "        Fit the k-prototypes model to the data to create clusters.\n",
        "        \"\"\"\n",
        "        self.data = self._prepare_data()\n",
        "        data_array = self.data.values\n",
        "        categorical_indices = [i for i, col in enumerate(self.data.columns) if self.data.dtypes[col] == 'object']\n",
        "\n",
        "        # Apply K-Prototypes clustering\n",
        "        kproto = KPrototypes(n_clusters=self.k, init='Cao', random_state=42)\n",
        "        clusters = kproto.fit_predict(data_array, categorical=categorical_indices)\n",
        "\n",
        "        # Store the cluster labels\n",
        "        self.cluster_labels = clusters\n",
        "\n",
        "        return clusters\n",
        "\n",
        "    def sample_indices(self):\n",
        "        \"\"\"\n",
        "        Sample the points that are the most representative based on distance from the cluster centers.\n",
        "\n",
        "        Returns:\n",
        "        - List of indices of the sampled data points.\n",
        "        \"\"\"\n",
        "        if self.cluster_labels is None:\n",
        "            self.fit_clusters()\n",
        "\n",
        "        # Get the indices of the cluster centers (medoids)\n",
        "        cluster_centers = self.data.iloc[np.unique(self.cluster_labels)].index\n",
        "\n",
        "        # Calculate distances between all points and their closest cluster center\n",
        "        distances = np.zeros(len(self.data))\n",
        "\n",
        "        for i, point in self.data.iterrows():\n",
        "            min_distance = np.min([self._calculate_distance(point, self.data.iloc[center]) for center in cluster_centers])\n",
        "            distances[i] = min_distance\n",
        "\n",
        "        # Determine the number of points to sample based on the sampling percentage\n",
        "        num_samples = max(1, int(len(self.data) * (self.sampling_percent / 100)))\n",
        "\n",
        "        # Get the indices of the top points with maximum distance from the centers\n",
        "        sampled_indices = np.argsort(distances)[-num_samples:]\n",
        "\n",
        "        return sampled_indices\n",
        "\n",
        "    def _calculate_distance(self, point1, point2):\n",
        "        \"\"\"\n",
        "        Calculate a distance between two data points, accounting for both categorical and numerical features.\n",
        "\n",
        "        Uses Euclidean distance for continuous features and a matching distance for categorical features.\n",
        "        \"\"\"\n",
        "        numeric_cols = self.data.select_dtypes(include=[np.number]).columns\n",
        "        categorical_cols = self.data.select_dtypes(include=['object']).columns\n",
        "\n",
        "        # Calculate numeric distance (Euclidean)\n",
        "        numeric_distance = np.linalg.norm(point1[numeric_cols] - point2[numeric_cols])\n",
        "\n",
        "        # Calculate categorical distance (matching)\n",
        "        categorical_distance = sum([1 if point1[col] != point2[col] else 0 for col in categorical_cols])\n",
        "\n",
        "        # Combine both distances\n",
        "        return numeric_distance + categorical_distance\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Execute the distance-based sampling process.\n",
        "\n",
        "        Returns:\n",
        "        - List of indices for sampled data points.\n",
        "        \"\"\"\n",
        "        self.fit_clusters()\n",
        "        return self.sample_indices()\n"
      ],
      "metadata": {
        "id": "1qgqGwih34r9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate continuous and categorical data\n",
        "num_samples = 1000\n",
        "num_continuous_features = 990\n",
        "num_categorical_features = 10\n",
        "continuous_data = np.random.randn(num_samples, num_continuous_features)\n",
        "categorical_data = np.random.choice(['A', 'B', 'C'], size=(num_samples, num_categorical_features))\n",
        "\n",
        "# Convert data to DataFrames\n",
        "continuous_df = pd.DataFrame(continuous_data, columns=[f\"cont_{i+1}\" for i in range(num_continuous_features)])\n",
        "categorical_df = pd.DataFrame(categorical_data, columns=[f\"cat_{i+1}\" for i in range(num_categorical_features)])\n",
        "\n",
        "# Concatenate to form complete dataset\n",
        "data = pd.concat([continuous_df, categorical_df], axis=1)\n",
        "\n",
        "# Initialize DistanceBasedSampler\n",
        "sampler = DistanceBasedSampler(data, k=5, sampling_percent=10)\n",
        "\n",
        "# Get indices of the sampled points\n",
        "sampled_indices = sampler.run()\n",
        "\n",
        "# Retrieve the sampled data from the original DataFrame\n",
        "sampled_data = data.iloc[sampled_indices]"
      ],
      "metadata": {
        "id": "ii7xYvim37Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sampled Data:\", len(sample_indices),\"Data:\",len(data))"
      ],
      "metadata": {
        "id": "0SXx5Tvy4GcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RANDON SAMPLING"
      ],
      "metadata": {
        "id": "q5w6E43q4zYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomSampler:\n",
        "    def __init__(self, data, sampling_percent=10):\n",
        "        \"\"\"\n",
        "        Initialize the RandomSampler class.\n",
        "\n",
        "        Parameters:\n",
        "        - data: DataFrame containing the dataset\n",
        "        - sampling_percent: Percentage of rows to sample (0-100)\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.sampling_percent = sampling_percent\n",
        "\n",
        "    def sample_indices(self):\n",
        "        \"\"\"\n",
        "        Randomly sample indices of rows from the dataset.\n",
        "\n",
        "        Returns:\n",
        "        - A list of sampled indices.\n",
        "        \"\"\"\n",
        "        if not (0 <= self.sampling_percent <= 100):\n",
        "            raise ValueError(\"sampling_percent must be between 0 and 100.\")\n",
        "\n",
        "        # Calculate the number of samples to draw\n",
        "        sample_size = int(len(self.data) * (self.sampling_percent / 100))\n",
        "        sample_size = max(1, sample_size)  # Ensure at least one index is selected\n",
        "\n",
        "        # Perform random sampling without replacement and return indices\n",
        "        sampled_indices = self.data.sample(n=sample_size, random_state=42).index.tolist()\n",
        "        return sampled_indices"
      ],
      "metadata": {
        "id": "lZclmsfb4vgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate continuous and categorical data\n",
        "num_samples = 1000\n",
        "num_continuous_features = 990\n",
        "num_categorical_features = 10\n",
        "continuous_data = np.random.randn(num_samples, num_continuous_features)\n",
        "categorical_data = np.random.choice(['A', 'B', 'C'], size=(num_samples, num_categorical_features))\n",
        "\n",
        "# Convert data to DataFrames\n",
        "continuous_df = pd.DataFrame(continuous_data, columns=[f\"cont_{i+1}\" for i in range(num_continuous_features)])\n",
        "categorical_df = pd.DataFrame(categorical_data, columns=[f\"cat_{i+1}\" for i in range(num_categorical_features)])\n",
        "\n",
        "# Concatenate to form complete dataset\n",
        "data = pd.concat([continuous_df, categorical_df], axis=1)\n",
        "\n",
        "\n",
        "sampler = RandomSampler(data=data, sampling_percent=10)\n",
        "sampled_indices = sampler.sample_indices()"
      ],
      "metadata": {
        "id": "AOd8--PJ41-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sampled Data:\", len(sample_indices),\"Data:\",len(data))"
      ],
      "metadata": {
        "id": "hu5dy-Lz5PLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MACHINE LEARNING MODELS"
      ],
      "metadata": {
        "id": "C74qxcZZ42Rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir Data"
      ],
      "metadata": {
        "id": "9M0kBWKKsi6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### READING DATA"
      ],
      "metadata": {
        "id": "DuZdfS3kN2OZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/Data/All Claims.csv')\n",
        "\n",
        "# Display the first few rows of the training data\n",
        "print(\"Training Data:\")\n",
        "# print(train.head())\n",
        "\n",
        "train.drop(columns=['id'],axis=1,inplace=True)\n",
        "\n",
        "X = train.drop(columns=['loss'])\n",
        "y = train['loss']\n",
        "\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "# Convert categorical features to category dtype\n",
        "for feature in categorical_features:\n",
        "    X[feature] = X[feature].astype('category')\n",
        "\n",
        "X.head()"
      ],
      "metadata": {
        "id": "5m4zGB5rsjZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "r-r4zOJIwkmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/Data/TUANDROMD.csv')\n",
        "\n",
        "# Display the first few rows of the training data\n",
        "print(\"Training Data:\")\n",
        "\n",
        "train = train[~(train['Label'].isnull())]\n",
        "train.columns = train.columns.str.replace(r'[^a-zA-Z0-9_]', '_', regex=True)\n",
        "\n",
        "X = train.drop(columns=['Label'])\n",
        "y = train['Label']\n",
        "\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "# Convert categorical features to category dtype\n",
        "for feature in categorical_features:\n",
        "    X[feature] = X[feature].astype('category')\n",
        "\n",
        "X.head()"
      ],
      "metadata": {
        "id": "JlHAGsBAGDLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)"
      ],
      "metadata": {
        "id": "XcXpRmC-Gvd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/Data/Andriod Permissions.csv')\n",
        "\n",
        "# Display the first few rows of the training data\n",
        "print(\"Training Data:\")\n",
        "\n",
        "train = train[~(train['Result'].isnull())]\n",
        "train.columns = train.columns.str.replace(r'[^a-zA-Z0-9_]', '_', regex=True)\n",
        "\n",
        "X = train.drop(columns=['Result'])\n",
        "y = train['Result']\n",
        "\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "# Convert categorical features to category dtype\n",
        "for feature in categorical_features:\n",
        "    X[feature] = X[feature].astype('category')\n",
        "\n",
        "X.head()"
      ],
      "metadata": {
        "id": "zdB8hq8MMg5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)"
      ],
      "metadata": {
        "id": "hA6tSbt5Mg1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "train = pd.read_csv('/content/Data/Student Success.csv', delimiter=';')\n",
        "\n",
        "# Clean and preprocess\n",
        "train = train[~train['Target'].isnull()]  # Remove rows with null Target\n",
        "train.columns = train.columns.str.replace(r'[^a-zA-Z0-9_]', '_', regex=True)  # Clean column names\n",
        "train = train[~(train['Target'] == 'Enrolled')]  # Remove 'Enrolled' from Target\n",
        "\n",
        "# Label encode the Target column\n",
        "label_encoder = LabelEncoder()\n",
        "train['Target'] = label_encoder.fit_transform(train['Target'])  # Encode Target as 0 and 1\n",
        "\n",
        "# Split features and target\n",
        "X = train.drop(columns=['Target'])\n",
        "y = train['Target']\n",
        "\n",
        "# Process categorical features\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "for feature in categorical_features:\n",
        "    X[feature] = X[feature].astype('category')\n",
        "\n",
        "X.head()"
      ],
      "metadata": {
        "id": "Ad2marcf9vbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)"
      ],
      "metadata": {
        "id": "1O_CaKiw-yRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/Data/Phishing URL.csv')\n",
        "\n",
        "train = train[[x for x in train.columns if x not in ['FILENAME','URL','Domain','Title']]]\n",
        "\n",
        "# Clean and preprocess\n",
        "train = train[~train['label'].isnull()]  # Remove rows with null Target\n",
        "train.columns = train.columns.str.replace(r'[^a-zA-Z0-9_]', '_', regex=True)  # Clean column names\n",
        "\n",
        "train = train.head(10000)\n",
        "# Split features and target\n",
        "X = train.drop(columns=['label'])\n",
        "y = train['label']\n",
        "\n",
        "# Process categorical features\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "for feature in categorical_features:\n",
        "    X[feature] = X[feature].astype('category')\n",
        "\n",
        "X.head()"
      ],
      "metadata": {
        "id": "CvU4MWgIm325"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)"
      ],
      "metadata": {
        "id": "JOBKeomjm357"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/Data/parkinsons_updrs.data')\n",
        "\n",
        "# Display the first few rows of the training data\n",
        "print(\"All Data:\")\n",
        "# print(train.head())\n",
        "\n",
        "train.drop('subject#', axis=1, inplace=True)\n",
        "train.drop('test_time', axis=1, inplace=True)\n",
        "train.drop('total_UPDRS', axis=1, inplace=True)\n",
        "\n",
        "X=train.drop('motor_UPDRS', axis=1)\n",
        "y=train[['motor_UPDRS']]\n",
        "\n",
        "# Replace positive and negative infinity with NaN across the DataFrame\n",
        "X = X.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "X.head()"
      ],
      "metadata": {
        "id": "Aa7Ui_dJ5-I9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "NHzs-DD-70Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/Data/creditcard.csv')\n",
        "\n",
        "# Display the first few rows of the training data\n",
        "print(\"Training Data:\")\n",
        "# print(train.head())\n",
        "\n",
        "print(train.shape)\n",
        "\n",
        "train.drop(columns=['Time'],axis=1)\n",
        "# test.drop(columns=['id'],axis=1)\n",
        "\n",
        "# Split the data into features (X) and target (y\n",
        "X = train.drop(columns=['Time','Class'])\n",
        "y = train['Class']\n",
        "\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "# Convert categorical features to category dtype\n",
        "for feature in categorical_features:\n",
        "    X[feature] = X[feature].astype('category')\n",
        "# Split the dataset into training and testing sets\n",
        "X.head()"
      ],
      "metadata": {
        "id": "ZlASYXu4OIBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "WEmO1CwBOIaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom transformer to drop highly correlated features\n",
        "class DropHighCorrelation(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, threshold=0.6):\n",
        "        \"\"\"\n",
        "        Custom transformer to drop highly correlated features.\n",
        "\n",
        "        Parameters:\n",
        "        - threshold: Correlation threshold above which features will be dropped.\n",
        "        \"\"\"\n",
        "        self.threshold = threshold\n",
        "        self.to_drop = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Identify features to drop based on the correlation threshold.\n",
        "        \"\"\"\n",
        "        corr_matrix = pd.DataFrame(X).corr().abs()  # Compute the absolute correlation matrix\n",
        "        upper_tri = corr_matrix.where(\n",
        "            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
        "        )  # Extract upper triangle\n",
        "        self.to_drop = [\n",
        "            column for column in upper_tri.columns if any(upper_tri[column] > self.threshold)\n",
        "        ]\n",
        "\n",
        "        print(f\"DropHighCorrelation: {len(self.to_drop)} columns will be dropped due to correlation (threshold={self.threshold}).\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Drop the identified features from the dataset.\n",
        "        \"\"\"\n",
        "        return pd.DataFrame(X).drop(columns=self.to_drop, errors='ignore')\n",
        "\n",
        "# Main ModelTrainer class\n",
        "class ModelTrainer:\n",
        "    def __init__(self, datasets_X, datasets_y, task_type, dataset_names=None):\n",
        "        \"\"\"\n",
        "        Initializes the ModelTrainer with datasets, explicitly provided task type, and dataset names.\n",
        "\n",
        "        Parameters:\n",
        "        - datasets_X: List of [X_train, X_test] for different datasets\n",
        "        - datasets_y: List of [y_train, y_test] for different datasets\n",
        "        - task_type: A string explicitly specifying the task type, either 'regression' or 'classification'.\n",
        "        - dataset_names: List of names for the datasets to be used as index in the results DataFrame\n",
        "        \"\"\"\n",
        "        if task_type not in ['classification', 'regression']:\n",
        "            raise ValueError(\"task_type must be either 'classification' or 'regression'\")\n",
        "\n",
        "        self.datasets_X = datasets_X\n",
        "        self.datasets_y = datasets_y\n",
        "        self.task_type = task_type\n",
        "        self.dataset_names = dataset_names\n",
        "\n",
        "    def _select_model(self):\n",
        "        \"\"\"Select model based on task type.\"\"\"\n",
        "        if self.task_type == 'classification':\n",
        "            return {\n",
        "                'RandomForest': RandomForestClassifier(random_state=42),\n",
        "                'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
        "                'LogisticRegression': LogisticRegression(),\n",
        "                'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "            }\n",
        "        elif self.task_type == 'regression':\n",
        "            return {\n",
        "                'RandomForest': RandomForestRegressor(random_state=42),\n",
        "                'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
        "                'LinearRegression': LinearRegression(),\n",
        "                'XGBoost': xgb.XGBRegressor(random_state=42)\n",
        "            }\n",
        "\n",
        "    def _create_pipeline(self, model, X):\n",
        "        \"\"\"Create a preprocessing and modeling pipeline.\"\"\"\n",
        "        # Identify categorical and numerical features\n",
        "        categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "        numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "        # Preprocessing for numerical features: Standard scaling\n",
        "        numerical_transformer = StandardScaler()\n",
        "\n",
        "        # Preprocessing for categorical features: One-hot encoding\n",
        "        categorical_transformer = OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)\n",
        "\n",
        "        # Combine preprocessors in a column transformer\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numerical_transformer, numerical_features),\n",
        "                ('cat', categorical_transformer, categorical_features)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Define a pipeline with preprocessing, correlation dropping, and the specified model\n",
        "        pipeline = Pipeline(steps=[\n",
        "            ('preprocessor', preprocessor),  # First preprocess\n",
        "            ('drop_high_corr', DropHighCorrelation(threshold=0.6)),  # Then drop highly correlated features\n",
        "            ('model', model)  # Finally, apply the model\n",
        "        ])\n",
        "        return pipeline\n",
        "\n",
        "    def _get_best_cutoff(self, y_true, y_pred_proba):\n",
        "        \"\"\"Use Youden's J statistic to determine the best cutoff point for classification.\"\"\"\n",
        "        fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
        "        youden_index = tpr - fpr\n",
        "        best_cutoff = thresholds[np.argmax(youden_index)]\n",
        "        return best_cutoff\n",
        "\n",
        "    def _train_and_evaluate(self, model, X_train, X_test, y_train, y_test):\n",
        "        \"\"\"Train the model and evaluate it on both the training and test datasets.\"\"\"\n",
        "        start_time = time.time()\n",
        "        model.fit(X_train, y_train)\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        y_pred_train = model.predict(X_train)\n",
        "        y_pred_test = model.predict(X_test)\n",
        "\n",
        "        if self.task_type == 'classification':\n",
        "            y_pred_proba_test = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred_test\n",
        "            y_pred_proba_train = model.predict_proba(X_train)[:, 1] if hasattr(model, 'predict_proba') else y_pred_train\n",
        "\n",
        "            best_cutoff = self._get_best_cutoff(y_test, y_pred_proba_test)\n",
        "            y_pred_class_test = (y_pred_proba_test >= best_cutoff).astype(int)\n",
        "            y_pred_class_train = (y_pred_proba_train >= best_cutoff).astype(int)\n",
        "\n",
        "            metrics = {\n",
        "                'Train F1': f1_score(y_train, y_pred_class_train),\n",
        "                'Test F1': f1_score(y_test, y_pred_class_test),\n",
        "                'Train AUC': roc_auc_score(y_train, y_pred_proba_train),\n",
        "                'Test AUC': roc_auc_score(y_test, y_pred_proba_test),\n",
        "                'Train Accuracy': accuracy_score(y_train, y_pred_class_train),\n",
        "                'Test Accuracy': accuracy_score(y_test, y_pred_class_test),\n",
        "                'Train Recall': recall_score(y_train, y_pred_class_train),\n",
        "                'Test Recall': recall_score(y_test, y_pred_class_test),\n",
        "                'Train Precision': precision_score(y_train, y_pred_class_train),\n",
        "                'Test Precision': precision_score(y_test, y_pred_class_test),\n",
        "                'Best Cutoff': best_cutoff,\n",
        "                'Training Time (seconds)': training_time\n",
        "            }\n",
        "        else:\n",
        "            metrics = {\n",
        "                'Train MSE': mean_squared_error(y_train, y_pred_train),\n",
        "                'Test MSE': mean_squared_error(y_test, y_pred_test),\n",
        "                'Train MAPE': mean_absolute_percentage_error(y_train, y_pred_train),\n",
        "                'Test MAPE': mean_absolute_percentage_error(y_test, y_pred_test),\n",
        "                'Train R2': r2_score(y_train, y_pred_train),\n",
        "                'Test R2': r2_score(y_test, y_pred_test),\n",
        "                'Training Time (seconds)': training_time\n",
        "            }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def train_models(self):\n",
        "        \"\"\"Train and evaluate models on multiple datasets and return a DataFrame of results.\"\"\"\n",
        "        models = self._select_model()\n",
        "        results = []\n",
        "\n",
        "        for idx, (X_data, y_data) in enumerate(zip(self.datasets_X, self.datasets_y)):\n",
        "            X_train, X_test = X_data\n",
        "            y_train, y_test = y_data\n",
        "\n",
        "            for model_name, model in models.items():\n",
        "                pipeline = self._create_pipeline(model, X_train)\n",
        "                metrics = self._train_and_evaluate(pipeline, X_train, X_test, y_train, y_test)\n",
        "                metrics['Dataset'] = self.dataset_names[idx] if self.dataset_names else f'Dataset {idx+1}'\n",
        "                metrics['Model'] = model_name\n",
        "                results.append(metrics)\n",
        "\n",
        "        return pd.DataFrame(results).set_index('Dataset')"
      ],
      "metadata": {
        "id": "LZegGUMVwmlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume you have the datasets as before\n",
        "trainer = ModelTrainer(\n",
        "    datasets_X=[[X_train1, X_test1],[X_train6, X_test6],[X_train7, X_test7]],\n",
        "    datasets_y=[[y_train1, y_test1],[y_train6, y_test6],[y_train7, y_test7]],\n",
        "    task_type='regression',\n",
        "    dataset_names=['All Claims','Parkinsons','Credit Card Fraud']\n",
        ")\n",
        "\n",
        "results = trainer.train_models()"
      ],
      "metadata": {
        "id": "rcrmOYAJ_JY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.to_csv('regression.csv',index=False)"
      ],
      "metadata": {
        "id": "AAgGLNzUA01Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "EAtDJYJqQfhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume you have the datasets as before\n",
        "trainer = ModelTrainer(\n",
        "    datasets_X=[[X_train2, X_test2],[X_train3, X_test3],[X_train4, X_test4],[X_train5, X_test5]],\n",
        "    datasets_y=[[y_train2, y_test2],[y_train3, y_test3],[y_train4, y_test4],[y_train5, y_test5]],\n",
        "    task_type='classification',\n",
        "    dataset_names=['Tuandromd','Andriod Permissions','Student Success','Phishing URL']\n",
        ")\n",
        "\n",
        "results = trainer.train_models()"
      ],
      "metadata": {
        "id": "JvLU-psNEFuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.to_csv('classification.csv',index=False)"
      ],
      "metadata": {
        "id": "rlKOi6LWaiJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "Dv8sIvFAQhmN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}